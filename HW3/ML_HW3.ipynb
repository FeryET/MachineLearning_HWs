{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_HW3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAt8CPhDTQnH"
      },
      "source": [
        "!pip install hmmlearn --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5pR-D1vOzBb"
      },
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from matplotlib import table as mpl_table\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "from scipy.stats import multivariate_normal\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC\n",
        "from hmmlearn import hmm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_PPUjayjE5T",
        "cellView": "form"
      },
      "source": [
        "#@title Run this to create Iris.csv { form-width: \"24px\" }\n",
        "%%writefile Iris.csv\n",
        "Sepal_Length,Sepal_Width,Petal_Length,Petal_Width,Class\n",
        "5.1,3.5,1.4,0.2,Iris-setosa\n",
        "4.9,3,1.4,0.2,Iris-setosa\n",
        "4.7,3.2,1.3,0.2,Iris-setosa\n",
        "4.6,3.1,1.5,0.2,Iris-setosa\n",
        "5,3.6,1.4,0.2,Iris-setosa\n",
        "5.4,3.9,1.7,0.4,Iris-setosa\n",
        "4.6,3.4,1.4,0.3,Iris-setosa\n",
        "5,3.4,1.5,0.2,Iris-setosa\n",
        "4.4,2.9,1.4,0.2,Iris-setosa\n",
        "4.9,3.1,1.5,0.1,Iris-setosa\n",
        "5.4,3.7,1.5,0.2,Iris-setosa\n",
        "4.8,3.4,1.6,0.2,Iris-setosa\n",
        "4.8,3,1.4,0.1,Iris-setosa\n",
        "4.3,3,1.1,0.1,Iris-setosa\n",
        "5.8,4,1.2,0.2,Iris-setosa\n",
        "5.7,4.4,1.5,0.4,Iris-setosa\n",
        "5.4,3.9,1.3,0.4,Iris-setosa\n",
        "5.1,3.5,1.4,0.3,Iris-setosa\n",
        "5.7,3.8,1.7,0.3,Iris-setosa\n",
        "5.1,3.8,1.5,0.3,Iris-setosa\n",
        "5.4,3.4,1.7,0.2,Iris-setosa\n",
        "5.1,3.7,1.5,0.4,Iris-setosa\n",
        "4.6,3.6,1,0.2,Iris-setosa\n",
        "5.1,3.3,1.7,0.5,Iris-setosa\n",
        "4.8,3.4,1.9,0.2,Iris-setosa\n",
        "5,3,1.6,0.2,Iris-setosa\n",
        "5,3.4,1.6,0.4,Iris-setosa\n",
        "5.2,3.5,1.5,0.2,Iris-setosa\n",
        "5.2,3.4,1.4,0.2,Iris-setosa\n",
        "4.7,3.2,1.6,0.2,Iris-setosa\n",
        "4.8,3.1,1.6,0.2,Iris-setosa\n",
        "5.4,3.4,1.5,0.4,Iris-setosa\n",
        "5.2,4.1,1.5,0.1,Iris-setosa\n",
        "5.5,4.2,1.4,0.2,Iris-setosa\n",
        "4.9,3.1,1.5,0.1,Iris-setosa\n",
        "5,3.2,1.2,0.2,Iris-setosa\n",
        "5.5,3.5,1.3,0.2,Iris-setosa\n",
        "4.9,3.1,1.5,0.1,Iris-setosa\n",
        "4.4,3,1.3,0.2,Iris-setosa\n",
        "5.1,3.4,1.5,0.2,Iris-setosa\n",
        "5,3.5,1.3,0.3,Iris-setosa\n",
        "4.5,2.3,1.3,0.3,Iris-setosa\n",
        "4.4,3.2,1.3,0.2,Iris-setosa\n",
        "5,3.5,1.6,0.6,Iris-setosa\n",
        "5.1,3.8,1.9,0.4,Iris-setosa\n",
        "4.8,3,1.4,0.3,Iris-setosa\n",
        "5.1,3.8,1.6,0.2,Iris-setosa\n",
        "4.6,3.2,1.4,0.2,Iris-setosa\n",
        "5.3,3.7,1.5,0.2,Iris-setosa\n",
        "5,3.3,1.4,0.2,Iris-setosa\n",
        "7,3.2,4.7,1.4,Iris-versicolor\n",
        "6.4,3.2,4.5,1.5,Iris-versicolor\n",
        "6.9,3.1,4.9,1.5,Iris-versicolor\n",
        "5.5,2.3,4,1.3,Iris-versicolor\n",
        "6.5,2.8,4.6,1.5,Iris-versicolor\n",
        "5.7,2.8,4.5,1.3,Iris-versicolor\n",
        "6.3,3.3,4.7,1.6,Iris-versicolor\n",
        "4.9,2.4,3.3,1,Iris-versicolor\n",
        "6.6,2.9,4.6,1.3,Iris-versicolor\n",
        "5.2,2.7,3.9,1.4,Iris-versicolor\n",
        "5,2,3.5,1,Iris-versicolor\n",
        "5.9,3,4.2,1.5,Iris-versicolor\n",
        "6,2.2,4,1,Iris-versicolor\n",
        "6.1,2.9,4.7,1.4,Iris-versicolor\n",
        "5.6,2.9,3.6,1.3,Iris-versicolor\n",
        "6.7,3.1,4.4,1.4,Iris-versicolor\n",
        "5.6,3,4.5,1.5,Iris-versicolor\n",
        "5.8,2.7,4.1,1,Iris-versicolor\n",
        "6.2,2.2,4.5,1.5,Iris-versicolor\n",
        "5.6,2.5,3.9,1.1,Iris-versicolor\n",
        "5.9,3.2,4.8,1.8,Iris-versicolor\n",
        "6.1,2.8,4,1.3,Iris-versicolor\n",
        "6.3,2.5,4.9,1.5,Iris-versicolor\n",
        "6.1,2.8,4.7,1.2,Iris-versicolor\n",
        "6.4,2.9,4.3,1.3,Iris-versicolor\n",
        "6.6,3,4.4,1.4,Iris-versicolor\n",
        "6.8,2.8,4.8,1.4,Iris-versicolor\n",
        "6.7,3,5,1.7,Iris-versicolor\n",
        "6,2.9,4.5,1.5,Iris-versicolor\n",
        "5.7,2.6,3.5,1,Iris-versicolor\n",
        "5.5,2.4,3.8,1.1,Iris-versicolor\n",
        "5.5,2.4,3.7,1,Iris-versicolor\n",
        "5.8,2.7,3.9,1.2,Iris-versicolor\n",
        "6,2.7,5.1,1.6,Iris-versicolor\n",
        "5.4,3,4.5,1.5,Iris-versicolor\n",
        "6,3.4,4.5,1.6,Iris-versicolor\n",
        "6.7,3.1,4.7,1.5,Iris-versicolor\n",
        "6.3,2.3,4.4,1.3,Iris-versicolor\n",
        "5.6,3,4.1,1.3,Iris-versicolor\n",
        "5.5,2.5,4,1.3,Iris-versicolor\n",
        "5.5,2.6,4.4,1.2,Iris-versicolor\n",
        "6.1,3,4.6,1.4,Iris-versicolor\n",
        "5.8,2.6,4,1.2,Iris-versicolor\n",
        "5,2.3,3.3,1,Iris-versicolor\n",
        "5.6,2.7,4.2,1.3,Iris-versicolor\n",
        "5.7,3,4.2,1.2,Iris-versicolor\n",
        "5.7,2.9,4.2,1.3,Iris-versicolor\n",
        "6.2,2.9,4.3,1.3,Iris-versicolor\n",
        "5.1,2.5,3,1.1,Iris-versicolor\n",
        "5.7,2.8,4.1,1.3,Iris-versicolor\n",
        "6.3,3.3,6,2.5,Iris-virginica\n",
        "5.8,2.7,5.1,1.9,Iris-virginica\n",
        "7.1,3,5.9,2.1,Iris-virginica\n",
        "6.3,2.9,5.6,1.8,Iris-virginica\n",
        "6.5,3,5.8,2.2,Iris-virginica\n",
        "7.6,3,6.6,2.1,Iris-virginica\n",
        "4.9,2.5,4.5,1.7,Iris-virginica\n",
        "7.3,2.9,6.3,1.8,Iris-virginica\n",
        "6.7,2.5,5.8,1.8,Iris-virginica\n",
        "7.2,3.6,6.1,2.5,Iris-virginica\n",
        "6.5,3.2,5.1,2,Iris-virginica\n",
        "6.4,2.7,5.3,1.9,Iris-virginica\n",
        "6.8,3,5.5,2.1,Iris-virginica\n",
        "5.7,2.5,5,2,Iris-virginica\n",
        "5.8,2.8,5.1,2.4,Iris-virginica\n",
        "6.4,3.2,5.3,2.3,Iris-virginica\n",
        "6.5,3,5.5,1.8,Iris-virginica\n",
        "7.7,3.8,6.7,2.2,Iris-virginica\n",
        "7.7,2.6,6.9,2.3,Iris-virginica\n",
        "6,2.2,5,1.5,Iris-virginica\n",
        "6.9,3.2,5.7,2.3,Iris-virginica\n",
        "5.6,2.8,4.9,2,Iris-virginica\n",
        "7.7,2.8,6.7,2,Iris-virginica\n",
        "6.3,2.7,4.9,1.8,Iris-virginica\n",
        "6.7,3.3,5.7,2.1,Iris-virginica\n",
        "7.2,3.2,6,1.8,Iris-virginica\n",
        "6.2,2.8,4.8,1.8,Iris-virginica\n",
        "6.1,3,4.9,1.8,Iris-virginica\n",
        "6.4,2.8,5.6,2.1,Iris-virginica\n",
        "7.2,3,5.8,1.6,Iris-virginica\n",
        "7.4,2.8,6.1,1.9,Iris-virginica\n",
        "7.9,3.8,6.4,2,Iris-virginica\n",
        "6.4,2.8,5.6,2.2,Iris-virginica\n",
        "6.3,2.8,5.1,1.5,Iris-virginica\n",
        "6.1,2.6,5.6,1.4,Iris-virginica\n",
        "7.7,3,6.1,2.3,Iris-virginica\n",
        "6.3,3.4,5.6,2.4,Iris-virginica\n",
        "6.4,3.1,5.5,1.8,Iris-virginica\n",
        "6,3,4.8,1.8,Iris-virginica\n",
        "6.9,3.1,5.4,2.1,Iris-virginica\n",
        "6.7,3.1,5.6,2.4,Iris-virginica\n",
        "6.9,3.1,5.1,2.3,Iris-virginica\n",
        "5.8,2.7,5.1,1.9,Iris-virginica\n",
        "6.8,3.2,5.9,2.3,Iris-virginica\n",
        "6.7,3.3,5.7,2.5,Iris-virginica\n",
        "6.7,3,5.2,2.3,Iris-virginica\n",
        "6.3,2.5,5,1.9,Iris-virginica\n",
        "6.5,3,5.2,2,Iris-virginica\n",
        "6.2,3.4,5.4,2.3,Iris-virginica\n",
        "5.9,3,5.1,1.8,Iris-virginica"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDmLzPtEPKrL"
      },
      "source": [
        "# Q9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG8UGiTiW1Ua"
      },
      "source": [
        "### Plotting Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So9QZz-3Pnju"
      },
      "source": [
        "viri_colors = cm.get_cmap(\"viridis\", 3)\n",
        "df = pd.read_csv(\"Iris.csv\", index_col=False)\n",
        "combs = list(combinations(df.columns[:-1], 2))\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df[\"Class\"])\n",
        "\n",
        "divisor = 2\n",
        "ncols = len(combs) // divisor\n",
        "nrows = len(combs) // ncols\n",
        "\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=np.array([ncols, nrows]) * 5)\n",
        "axes = list(axes.flat)\n",
        "\n",
        "for idx, (l1, l2) in enumerate(combs):\n",
        "  lines = []\n",
        "  ax = axes[idx]\n",
        "  for c_idx, class_ in enumerate(encoder.classes_):\n",
        "    sub_df = df.loc[df[\"Class\"] == class_]\n",
        "    y = encoder.transform(sub_df[\"Class\"])\n",
        "    l = ax.scatter(\n",
        "        sub_df[l1], sub_df[l2], color=viri_colors(c_idx), label=class_\n",
        "    )\n",
        "    lines.append(l)\n",
        "  ax.set_title(f\"({idx+1})\")\n",
        "  ax.set_xlabel(l1)\n",
        "  ax.set_ylabel(l2)\n",
        "  # ax.set_aspect(\"equal\")\n",
        "\n",
        "fig.legend(\n",
        "    lines, encoder.classes_, loc=(0.34, 0.92), ncol=3,\n",
        ")\n",
        "fig.suptitle(\"2D Feature Plots\", fontsize=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYd-xcXjW651"
      },
      "source": [
        "### Fitting GMMs for each set of features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ1rFMZJWsWR"
      },
      "source": [
        "def generate_cov(n):\n",
        "  mat = np.eye(n)\n",
        "  # for i, j in combinations(range(n), 2):\n",
        "    # mat[j, i] = mat[i, j] = np.clip(np.random.normal(), 0, 0.5)\n",
        "  # print(mat)\n",
        "  return mat\n",
        "\n",
        "eps = 10e-5\n",
        "\n",
        "class GMM:\n",
        "  def __init__(self, n_classes, progress=False, tol=10e-10, max_iter=10e3):\n",
        "    self.n_classes = n_classes\n",
        "    self.progress = progress\n",
        "    self.tol = tol\n",
        "    self.max_iter = max_iter\n",
        "    \n",
        "    \n",
        "  \n",
        "  def fit(self, X):\n",
        "    reg_cov = 10e-6 * np.identity(X.shape[1])\n",
        "    eps = 10e-10\n",
        "\n",
        "    N = X.shape[0]\n",
        "    n_features = X.shape[1]\n",
        "\n",
        "    ones = np.ones((n_features,))\n",
        "    \n",
        "    KM = KMeans(n_clusters=self.n_classes)\n",
        "    Z = KM.fit_predict(X)\n",
        "\n",
        "    self.alpha = ones/self.n_classes\n",
        "    # self.mu = np.random.normal(scale=1, size=(self.n_classes,n_features))\n",
        "    self.mu = KM.cluster_centers_\n",
        "    self.cov = [np.eye(n_features) for i in range(self.n_classes)]\n",
        "    \n",
        "\n",
        "    Q = np.zeros((N, self.n_classes))\n",
        "    Q[np.arange(N), Z] = 1\n",
        "\n",
        "    old_ll = -np.inf\n",
        "    iter = 0\n",
        "    while True:\n",
        "      ### M step\n",
        "      Q_sum = Q.sum(axis=0) + eps\n",
        "      self.alpha = Q_sum/N\n",
        "      self.mu = (X.T @ Q / Q_sum).T\n",
        "      for j in range(self.n_classes):\n",
        "        for i in range(N):\n",
        "            x_mu = (X[i,:] - self.mu[j, :]).reshape(-1, 1)\n",
        "            self.cov[j] += Q[i, j] * (x_mu @ x_mu.T)\n",
        "        self.cov[j] /= Q_sum[j]\n",
        "        self.cov[j] += reg_cov\n",
        "\n",
        "      ### E step\n",
        "      # reg_cov will prevent cov matrix from going singular\n",
        "      \n",
        "      # computing probabilites for each submodel\n",
        "      for j in range(self.n_classes):\n",
        "          mean = self.mu[j, :] \n",
        "          cov = self.cov[j]\n",
        "          Q[:, j] = self.alpha[j] * multivariate_normal.pdf(X,mean=mean , cov=cov)\n",
        "      # normalizaing\n",
        "      Q = Q / (Q.sum(axis=-1, keepdims=True))\n",
        "        \n",
        "      \n",
        "      # computing log likelihood\n",
        "      ll = 0\n",
        "      for j in range(self.n_classes):\n",
        "        ll += (np.log(\n",
        "            self.alpha[j] + eps\n",
        "        ) * Q[:, j]).sum()\n",
        "\n",
        "        mean, cov = self.mu[j], self.cov[j]\n",
        "        p_x = multivariate_normal.pdf(\n",
        "                X, mean, cov, allow_singular = True\n",
        "            )\n",
        "        P = self.alpha[j] * p_x\n",
        "        ll += (np.log(p_x+eps) * Q[:, j]).sum()\n",
        "      ll = ll / N\n",
        "\n",
        "      if self.progress:\n",
        "        print(f\"ITER: {i}, LL: {ll}\")\n",
        "\n",
        "      if np.abs(ll - old_ll) < self.tol or iter >= self.max_iter:\n",
        "          break\n",
        "      old_ll = ll\n",
        "      iter += 1\n",
        "\n",
        "    return self\n",
        "  \n",
        "\n",
        "  def predict(self, X):\n",
        "    def softmax(A):\n",
        "      expA = np.exp(A)\n",
        "      return expA/expA.sum(axis=-1)\n",
        "    N = X.shape[0]\n",
        "    probs = np.zeros((N, self.n_classes))\n",
        "    for j in range(self.n_classes):\n",
        "      mean, cov = self.mu[j], self.cov[j]\n",
        "      probs[:,j] = self.alpha[j] * multivariate_normal.pdf(X, mean=mean, cov=cov)\n",
        "    # probs = softmax(probs)\n",
        "    return np.argmax(probs, axis=-1)\n",
        "\n",
        "  def score(self, X):\n",
        "    assert X.shape[1] == self.mu.shape[1]\n",
        "    N = X.shape[0]\n",
        "    Q = np.ones((N, self.n_classes), dtype=np.float) / self.n_classes\n",
        "    for j in range(self.n_classes):\n",
        "          mean = self.mu[j, :] \n",
        "          cov = self.cov[j]\n",
        "          Q[:, j] = self.alpha[j] * multivariate_normal.pdf(X,mean=mean , cov=cov)\n",
        "    # normalizing\n",
        "    Q = Q / (Q.sum(axis=-1, keepdims=True))\n",
        "    \n",
        "    # computing log likelihood\n",
        "    ll = 0\n",
        "    for j in range(self.n_classes):\n",
        "      ll += (np.log(\n",
        "            self.alpha[j] + eps\n",
        "      ) * Q[:, j]).sum()\n",
        "\n",
        "      mean, cov = self.mu[j], self.cov[j]\n",
        "      p_x = multivariate_normal.pdf(\n",
        "                X, mean, cov, allow_singular = True\n",
        "      )\n",
        "      P = self.alpha[j] * p_x\n",
        "      ll += (np.log(p_x+eps) * Q[:, j]).sum()\n",
        "    ll = ll / N\n",
        "    return ll  \n",
        "\n",
        "  def map_true_classes(self, X, y, preds):\n",
        "    preds = self.predict(X)\n",
        "    class_ids = np.unique(y)\n",
        "    self.cls_map = {}\n",
        "    for cls_id in class_ids:\n",
        "      indices = (y == cls_id)\n",
        "      x = X[indices, :]\n",
        "      mean_dists = []\n",
        "      for m in self.mu:\n",
        "        mean_dists.append(np.sqrt(((x - m)**2).sum(axis=-1)).mean())\n",
        "      self.cls_map[np.argmin(mean_dists)] = cls_id\n",
        "\n",
        "    return np.array([self.cls_map.get(p, -1) for p in preds])\n",
        "\n",
        "  def _n_parameters(self):\n",
        "    # number of mean params \n",
        "    n_mean = np.prod(self.mu.shape)\n",
        "    # number of cov params, symmetrical so half of them are non params\n",
        "    n_cov = np.sum([np.prod(c.shape) for c in self.cov])/2\n",
        "    return  n_mean + n_cov + self.n_classes\n",
        "\n",
        "  def aic(self, X):\n",
        "    return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n",
        "  def bic(self, X):\n",
        "    return -2 * self.score(X) * X.shape[0] + self._n_parameters() * np.log(X.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2Iq3PoW4CUY"
      },
      "source": [
        "### Parts B, C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYkF48H3sRAE"
      },
      "source": [
        "models = []\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df[\"Class\"])\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=np.array([ncols, nrows]) * 5)\n",
        "axes = list(axes.flat)\n",
        "scaler = StandardScaler()\n",
        "errors = []\n",
        "for idx, (l1, l2) in enumerate(combs):\n",
        "  ax = axes[idx]\n",
        "  \n",
        "  \n",
        "  # Training model\n",
        "  gmm = GMM(3)\n",
        "  models.append(gmm)\n",
        "  X = df[[l1, l2]].to_numpy()\n",
        "  gmm.fit(X)\n",
        "  \n",
        "  # creating mesh\n",
        "  xx, yy = np.meshgrid(np.sort(X[:, 0]), np.sort(X[:, 1]))\n",
        "  mesh = np.hstack((xx.reshape(-1, 1), yy.reshape(-1, 1)))\n",
        "\n",
        "\n",
        "  lines = []\n",
        "  # scattering datapoints\n",
        "  for c_idx, class_ in enumerate(encoder.classes_):\n",
        "    sub_df = df.loc[df[\"Class\"] == class_]\n",
        "    x = sub_df[[l1, l2]].to_numpy()\n",
        "    l = ax.scatter(\n",
        "        x[:, 0], x[:, 1], color=viri_colors(c_idx), label=class_\n",
        "    )\n",
        "    lines.append(l)\n",
        "\n",
        "  # plotting contours\n",
        "  for m,c in zip(gmm.mu,gmm.cov):\n",
        "    multi_normal = multivariate_normal(mean=m,cov=c)\n",
        "    ax.contour(np.sort(X[:,0]),np.sort(X[:,1]),multi_normal.pdf(mesh).reshape(X.shape[0], X.shape[0]),colors='firebrick',alpha=0.3)\n",
        "    ax.scatter(m[0],m[1],c='b',zorder=10,s=100, marker=\"+\")\n",
        "    \n",
        "  ax.set_xlabel(l1)\n",
        "  ax.set_ylabel(l2)\n",
        "  ax.set_title(f\"({idx+1})\")\n",
        "  # computing error\n",
        "  preds = gmm.predict(X)\n",
        "  true_preds = gmm.map_true_classes(X, y, preds)\n",
        "  err = (true_preds != y).astype(np.int).mean()\n",
        "  errors.append(err)\n",
        "  print(f\"{idx+1} completed\")\n",
        "\n",
        "# deploying a legend\n",
        "fig.legend(\n",
        "    lines, encoder.classes_ + [\"Centroids\"], loc=(0.25, 0.92), ncol=3,\n",
        ")\n",
        "fig.suptitle(\"2D Feature Plots\", fontsize=20)\n",
        "\n",
        "\n",
        "\n",
        "# saving results for partc\n",
        "result_part_c = []\n",
        "for e, c in zip(errors, combs):\n",
        "  result_part_c.append(\n",
        "      {\n",
        "          \"Feature Combination\": \" || \".join(c),\n",
        "          \"Error\": e\n",
        "      }\n",
        "  )\n",
        "\n",
        "result_part_c = pd.DataFrame(result_part_c)\n",
        "result_part_c.sort_values(by=\"Error\", inplace=True)\n",
        "result_part_c.to_csv(\"q9_errors_part_c.csv\", index=False)\n",
        "result_part_c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ParS26Vx7SJ6"
      },
      "source": [
        "# Plotting Means and Covariances \n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=np.array([3.2, 2.2]) * 4)\n",
        "axes = list(axes.flat)\n",
        "[ax.axis(\"off\") for ax in axes]\n",
        "for idx, ax in enumerate(axes):\n",
        "  gmm = models[idx]\n",
        "  means = gmm.mu\n",
        "  cov = gmm.cov\n",
        "  mean_text = [[f\"{x:.2f}\" for x in row] for row in list(means)]\n",
        "  \n",
        "  rowlabels = ['Mean_G1','Mean_G2', 'Mean_G3']\n",
        "  collabels = ['F1','F2']\n",
        "  rcolors = cm.BuPu(np.full(len(rowlabels), 0.1))\n",
        "  ccolors = cm.BuPu(np.full(len(collabels), 0.1))\n",
        "  \n",
        "  table = ax.table(cellText=mean_text, loc=\"upper center\", bbox=(0.25,0.25, 0.5,0.5),\n",
        "               rowLabels=rowlabels, colLabels=collabels, cellLoc=\"center\",\n",
        "           rowColours=rcolors, colColours=ccolors)\n",
        "  table.auto_set_font_size()\n",
        "\n",
        "  locations = [\n",
        "               \"lower left\", \"lower center\", \"lower right\"\n",
        "  ]\n",
        "  \n",
        "  ax.set_title(f\"Means of GMM ({idx+1})\")\n",
        "fig.suptitle(\"Comparison of Means\", fontsize=20)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=np.array([3, 3.5]) * 3.5)\n",
        "axes = list(axes.flat)\n",
        "[ax.axis(\"off\") for ax in axes]\n",
        "for idx, ax in enumerate(axes):\n",
        "  gmm = models[idx]\n",
        "  cov = gmm.cov\n",
        "  bboxes = [\n",
        "            [0.3, 0.03, 0.4, 0.3],\n",
        "            [0.3, 0.36, 0.4, 0.3],\n",
        "            [0.3, 0.7, 0.4, 0.3],\n",
        "  ]\n",
        "\n",
        "  rowlabels = ['F1','F2']\n",
        "  collabels = ['F1','F2']\n",
        "  rcolors = cm.BuPu(np.full(len(rowlabels), 0.1))\n",
        "  ccolors = cm.BuPu(np.full(len(collabels), 0.1))\n",
        "  \n",
        "  cov_text = [[[f\"{x:.2f}\" for x in row] for row in c] for c in cov]\n",
        "  for r, (c, b) in enumerate(zip(cov_text, bboxes)):\n",
        "    table1 = ax.table(cellText=c, bbox = b, rowLabels=rowlabels, colLabels=collabels, cellLoc=\"center\",\n",
        "           rowColours=rcolors, colColours=ccolors)\n",
        "    table1.auto_set_font_size()\n",
        "  \n",
        "  ax.set_title(f\"Covariances of GMM ({idx+1})\")\n",
        "fig.suptitle(\"Comparison of Covariances\", fontsize=20)\n",
        "  \n",
        "# Plotting Errros\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ind = np.arange(1,len(result_part_c)+1)\n",
        "ax.plot(ind, result_part_c[\"Error\"], label=\"Error Rate\")\n",
        "ax.grid(\"on\",linestyle=\"--\", linewidth=0.5)\n",
        "ax.set_xticks(ind)\n",
        "ax.set_xticklabels([\" || \".join([\"_\".join(y[0] for y in x.split(\"_\")) for x in s.split(\" || \")]) for s in result_part_c[\"Feature Combination\"]], rotation=45)\n",
        "ax.set_xlabel(\"GMM\")\n",
        "ax.set_ylabel(\"Error Rate\")\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQHHmN988aD_"
      },
      "source": [
        "### Part D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SDblg51541N"
      },
      "source": [
        "\n",
        "cv_combs = []\n",
        "for r in range(2, len(df.columns[:-1]) + 1):\n",
        "  cv_combs.extend(\n",
        "      list(\n",
        "          combinations(df.columns[:-1], r)\n",
        "      )\n",
        "  )\n",
        "\n",
        "result_df = []\n",
        "for comb in cv_combs:\n",
        "  print(comb)\n",
        "  X = df[[c for c in comb]].to_numpy()\n",
        "  y = encoder.fit_transform(df[\"Class\"])\n",
        "  gmm = GMM(3)\n",
        "  gmm.fit(X)\n",
        "  preds = gmm.predict(X)\n",
        "  true_preds = gmm.map_true_classes(X, y, preds)\n",
        "  err = (y != true_preds).astype(np.int).mean()\n",
        "  result_df.append(\n",
        "      {\n",
        "          \"Combination\": \" || \".join([\"_\".join([d[0] for d in c.split(\"_\")]) for c in comb]),\n",
        "          \"Num Features\": len(comb),\n",
        "          \"Error\": err,\n",
        "          \"aic\": gmm.aic(X),\n",
        "          \"bic\": gmm.bic(X),\n",
        "      }\n",
        "  )\n",
        "\n",
        "result_df = pd.DataFrame(result_df)\n",
        "# result_df.sort_values(by=\"Error\", inplace=True)\n",
        "result_df.to_csv(\"q9_partd.csv\",index=False)\n",
        "\n",
        "fig, axes = plt.subplots(2, 1 , figsize=(10, 12),sharey=True)\n",
        "ylabels = [\"aic\", \"bic\"]\n",
        "\n",
        "for ax, l in zip(axes, ylabels):\n",
        "  result_df.plot(x=\"Combination\", y=l, ax=ax)\n",
        "  ax.set_xticks(np.arange(len(result_df.Combination)))\n",
        "  ax.set_xticklabels(\n",
        "      result_df.Combination, rotation=45\n",
        "  )\n",
        "  ax.set_title(f\"{l.upper()} for Different Feature Comibnations\")\n",
        "  ax.grid(\"on\",linestyle=\"--\", linewidth=0.5)\n",
        "fig.suptitle(\"Metrics for Different Feature Comibnations\", fontsize=18)\n",
        "fig.tight_layout(pad=3.0, rect=(0.02, 0.02, 0.98, 0.95))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "l = \"Error\"\n",
        "\n",
        "result_df.plot(x=\"Combination\", y=l, ax=ax)\n",
        "ax.set_xticks(np.arange(len(result_df.Combination)))\n",
        "ax.set_xticklabels(\n",
        "    result_df.Combination, rotation=45\n",
        ")\n",
        "ax.set_ylim(0, result_df[\"Error\"].max() * 1.1)\n",
        "ax.set_yticks(np.arange(0, result_df[\"Error\"].max() * 1.1, 0.025))\n",
        "ax.set_title(f\"{l.upper()} for Different Feature Comibnations\")\n",
        "ax.grid(\"on\",linestyle=\"--\", linewidth=0.5)\n",
        "fig.tight_layout(pad=3.0, rect=(0.02, 0.02, 0.98, 0.95))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy49o2ODSYuN"
      },
      "source": [
        "# Q11"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys-cp4bpOpEk"
      },
      "source": [
        "# Part A\n",
        "\n",
        "states = [\"snowy\", \"rainy\", \"sunny\"]\n",
        "observations = [\"tshirt\", \"coat\", \"umbrella\"]\n",
        "\n",
        "n_states = 3\n",
        "model = hmm.MultinomialHMM(n_components = n_states, init_params= \"\", params= \"\")\n",
        "\n",
        "model.startprob_ = np.array([1, 1, 1], dtype=np.float)/3\n",
        "\n",
        "model.transmat_ = np.array([\n",
        "                            [0.2, 0.05, 0.75],\n",
        "                            [0.02, 0.6, 0.38],\n",
        "                            [0.05, 0.15, 0.8],\n",
        "                            ])\n",
        "        \n",
        "\n",
        "\n",
        "model.emissionprob_ = np.array([\n",
        "        [0, 0.5, 0.5],\n",
        "        [0.05, 0.3, 0.65],\n",
        "        [0.6, 0.3, 0.1],\n",
        "        ])\n",
        "print(model.emissionprob_.shape)\n",
        "\n",
        "# PART B\n",
        "seq = np.array([2,1,1,1,2,2])\n",
        "i = seq[0]\n",
        "prob = model.startprob_[i]\n",
        "for j in seq[1:]:\n",
        "  prob *= model.transmat_[i, j]\n",
        "  i = j\n",
        "print(prob)\n",
        "\n",
        "# PART C\n",
        "seq = np.array([[1,1,2,2,0,2,1]]).T\n",
        "model.fit(seq, lengths=[seq.size])\n",
        "logprob, state_seq = model.decode(seq, algorithm=\"viterbi\")\n",
        "print([states[i] for i in state_seq], np.exp(logprob))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhMcbNosXiO3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}